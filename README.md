# ResNet-Model-
Machine faults classification using spectrogram images and ResNet model
Transfer learning is a technique that utilizes a trained model's knowledge to learn another set of data. Transfer learning aims to improve learning in the target domain by leveraging knowledge from the source domain and learning task. In this technique, a model trained on one task is used as the starting point for a model on a second task. This can be useful when the second task is like the first task, or when there is limited data available for the second task. By using the learned features from the first task as a starting point, the model can learn more quickly and effectively on the second task. This can also help to prevent overfitting, as the model will have already learned general features that are likely to be useful in the second task.
A.	Residual Network: To solve the problem of the vanishing/exploding gradient, in this model Residual Blocks is introduced. In this network, we use a technique called skip connections. The skip connection connects activations of a layer to further layers by skipping some layers in between. This forms a residual block and the Resnets are made by stacking these residual blocks together. The approach behind this network is instead of layers learning the underlying mapping, we allow the network to fit the residual mapping. The advantage of adding this type of skip connection is that if any layer hurt the performance of architecture, then it will be skipped by regularization. So, this results in training a very deep neural network without the problems caused by vanishing/exploding gradient.  
